# -*- coding: utf-8 -*-
"""WS_API 4_20.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11AKQHgXu8pF-AcaAa0tk8WiGuUk3Zy05

#Web Scrape Section
"""

!pip install bs4
from bs4 import BeautifulSoup
from urllib.request import urlopen
import re
import pandas as pd
import pprint

html = urlopen('https://socialsciences.cornell.edu/help-center')
bs_html = BeautifulSoup(html, 'html.parser')
print(bs_html)

#find_all includes bios
consultants = bs_html.find_all('div', {'class':'accordion__item js-accordion__item paragraph paragraph--type--faq-fields paragraph--view-mode--default'})
print(consultants)
len(consultants)

#Print the text each consultants
for content in consultants:
    print(content.get_text())

print(consultants[1].get_text())

!pip install python-docx 
import docx

#Create prep file names
string = "file"
number = 0
end = ".docx"
#filename = string + str(number)

len(consultants)

doc = docx.Document()
while number < len(consultants):
# Add a Title to the document 
  doc.add_heading('CCSS Consultant', 0)
  doc.add_paragraph(consultants[number].get_text())
  print(consultants[number].get_text())
  filename = string + str(number) + end
  print(filename)
  doc.save(filename)
  number = number + 1

"""#API Section

List of API fields available to pull from.
[Reddit fields](https://www.reddit.com/dev/api/#fullnames)

What the API data looks like. The data you are pulling from. 
[Reddit API data](https://www.reddit.com/r/python/top.json?limit=100&t=year)

Below I will provide sample code but not my credentials. You may input yours in when prompted.
"""

!pip install requests
#Import libraries
import requests

#input credentials for access and secret. 
access = ''
secret = ''

# note that CLIENT_ID refers to 'personal use script' and SECRET_TOKEN to 'token'
auth = requests.auth.HTTPBasicAuth('', '')

# here we pass our login method (password), username, and password
data = {'grant_type': 'password',
        'username': '',
        'password': ''}

# setup our header info, which gives reddit a brief description of our app
headers = {'User-Agent': 'MyBot/0.0.1'}

# send our request for an OAuth token
res = requests.post('https://www.reddit.com/api/v1/access_token',
                    auth=auth, data=data, headers=headers)

# convert response to JSON and pull access_token value
TOKEN = res.json()['access_token']

# add authorization to our headers dictionary
headers = {**headers, **{'Authorization': f"bearer {TOKEN}"}}

# while the token is valid (~2 hours) we just add headers=headers to our requests
requests.get('https://oauth.reddit.com/api/v1/me', headers=headers)

#Pull a request. Trending posts related to python. 
res = requests.get("https://oauth.reddit.com/r/python/hot",
                   headers=headers)

print(res.json())  # let's see what we get. Similar to the webpage URL from the beginning.

for post in res.json()['data']['children']:
    print(post['data']['title'])

# make a request for the most recent posts in /r/Python. instead of hot this time new
res = requests.get("https://oauth.reddit.com/r/python/new",
                   headers=headers)

df = pd.DataFrame()  # initialize dataframe
#URL below section 'Pulling all the most recent posts in a subreddit and creating a local database' gray box shows fields allowed to pull. 
#https://brentgaisford.medium.com/how-to-use-python-and-the-reddit-api-to-build-a-local-database-of-reddit-posts-and-comments-ca9f3843bfc2

# loop through each post retrieved from GET request
for post in res.json()['data']['children']:
    # append relevant data to dataframe
    df = df.append({
        'subreddit': post['data']['subreddit'],
        'title': post['data']['title'],
        'selftext': post['data']['selftext'],
        'upvote_ratio': post['data']['upvote_ratio'],
        'ups': post['data']['ups'],
        'downs': post['data']['downs'],
        'score': post['data']['score'],
        'author': post['data']['author'],
        'comments_count': post['data']['num_comments']
    }, ignore_index=True)

display(df)
#Export to csv

!pip install praw
import praw

#Input credentials. client_id is access token. 'user_agent' is name of developer account. 
reddit = praw.Reddit(client_id='',
                     client_secret='', password='',
                     user_agent='', username='')

#Create empty lists. This is where data will be stored
text = []

string = "sub"
number = 0
end = ".docx"

subreddit = reddit.subreddit('worldnews')
hot_post = subreddit.hot(limit = 10)
for sub in hot_post:
  text.append(sub.title)
  doc = docx.Document()
# Add a Title to the document 
  doc.add_heading('Subreddit', 0)
  doc.add_paragraph(sub.title)
  filename = string + str(number) + end
  doc.save(filename)
  number = number + 1


print(text)

!pip install wikipedia
#Wikipedia API for Python uses a wrapper to easier extract content from user side. 
import wikipedia

#Set language
wikipedia.set_lang("en")

print(wikipedia.summary("COVID-19 pandemic in Canada", sentences=3))